{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install all required packages if not already installed\n",
                "!pip install pandas matplotlib seaborn scikit-learn xgboost joblib -q"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import os\n",
                "import joblib\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
                "from sklearn.metrics import (\n",
                "    accuracy_score, precision_score, recall_score, f1_score,\n",
                "    matthews_corrcoef, roc_auc_score,\n",
                "    classification_report, confusion_matrix\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Data Loading & Exploration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the dataset\n",
                "car_df = pd.read_csv('global_cars_enhanced.csv')\n",
                "\n",
                "# Display first few rows\n",
                "print('First 5 rows:')\n",
                "print(car_df.head())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Dataset info\n",
                "print(car_df.info())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check for missing values\n",
                "print('Missing values:')\n",
                "print(car_df.isnull().sum())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Summary statistics\n",
                "print('Summary statistics:')\n",
                "print(car_df.describe())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize Price_Category distribution\n",
                "plt.figure(figsize=(10, 6))\n",
                "sns.countplot(x='Price_Category', data=car_df)\n",
                "plt.title('Distribution of Car Price Category')\n",
                "plt.show()\n",
                "print(car_df['Price_Category'].value_counts())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Drop Car_ID column (not useful for classification)\n",
                "car_df = car_df.drop('Car_ID', axis=1)\n",
                "\n",
                "# Drop Price_USD column (would leak the target)\n",
                "if 'Price_USD' in car_df.columns:\n",
                "    car_df = car_df.drop('Price_USD', axis=1)\n",
                "\n",
                "# Encode all remaining categorical (object) columns using LabelEncoder\n",
                "label_encoders = {}\n",
                "for col in car_df.select_dtypes(include=['object']).columns:\n",
                "    if col == 'Price_Category':\n",
                "        continue\n",
                "    le = LabelEncoder()\n",
                "    car_df[col] = le.fit_transform(car_df[col])\n",
                "    label_encoders[col] = le\n",
                "    print(f'Encoded column: {col}')\n",
                "\n",
                "print('\\nDataFrame after encoding:')\n",
                "print(car_df.head())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define features (X) and target (y)\n",
                "X = car_df.drop('Price_Category', axis=1)\n",
                "y = car_df['Price_Category']\n",
                "\n",
                "# Encode the target variable for metrics that need numerical labels\n",
                "le_target = LabelEncoder()\n",
                "y_encoded = le_target.fit_transform(y)\n",
                "class_names = le_target.classes_\n",
                "print(f'Target classes: {class_names}')\n",
                "print(f'Encoded values: {np.unique(y_encoded)}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Split data into training and testing sets\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
                ")\n",
                "\n",
                "# Scale features\n",
                "scaler = StandardScaler()\n",
                "X_train_scaled = scaler.fit_transform(X_train)\n",
                "X_test_scaled = scaler.transform(X_test)\n",
                "\n",
                "feature_names = X.columns.tolist()\n",
                "\n",
                "print(f'X_train shape: {X_train_scaled.shape}')\n",
                "print(f'X_test shape: {X_test_scaled.shape}')\n",
                "print(f'y_train shape: {y_train.shape}')\n",
                "print(f'y_test shape: {y_test.shape}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create Models directory and save preprocessing artifacts\n",
                "os.makedirs('Models', exist_ok=True)\n",
                "\n",
                "# Save the scaler, label encoders, and target encoder for the Streamlit app\n",
                "joblib.dump(scaler, 'Models/scaler.joblib')\n",
                "joblib.dump(label_encoders, 'Models/label_encoders.joblib')\n",
                "joblib.dump(le_target, 'Models/target_encoder.joblib')\n",
                "joblib.dump(feature_names, 'Models/feature_names.joblib')\n",
                "\n",
                "print('Preprocessing artifacts saved to Models/ folder.')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Helper Function for Evaluation Metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Dictionary to store results for comparison\n",
                "results = {}\n",
                "\n",
                "def evaluate_model(model_name, model, X_test, y_test):\n",
                "    \"\"\"\n",
                "    Evaluate a trained model and print all required metrics.\n",
                "    Returns a dictionary of metric values.\n",
                "    \"\"\"\n",
                "    y_pred = model.predict(X_test)\n",
                "    y_pred_proba = model.predict_proba(X_test)\n",
                "\n",
                "    # 1. Accuracy\n",
                "    acc = accuracy_score(y_test, y_pred)\n",
                "\n",
                "    # 2. AUC Score (One-vs-Rest for multi-class)\n",
                "    auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='weighted')\n",
                "\n",
                "    # 3. Precision (weighted average for multi-class)\n",
                "    prec = precision_score(y_test, y_pred, average='weighted')\n",
                "\n",
                "    # 4. Recall (weighted average for multi-class)\n",
                "    rec = recall_score(y_test, y_pred, average='weighted')\n",
                "\n",
                "    # 5. F1 Score (weighted average for multi-class)\n",
                "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
                "\n",
                "    # 6. Matthews Correlation Coefficient\n",
                "    mcc = matthews_corrcoef(y_test, y_pred)\n",
                "\n",
                "    # Print results\n",
                "    print(\"=\" * 55)\n",
                "    print(f\"  {model_name} - EVALUATION RESULTS\")\n",
                "    print(\"=\" * 55)\n",
                "    print(f\"  Accuracy  : {acc:.4f} ({acc*100:.2f}%)\")\n",
                "    print(f\"  AUC Score : {auc:.4f}\")\n",
                "    print(f\"  Precision : {prec:.4f}\")\n",
                "    print(f\"  Recall    : {rec:.4f}\")\n",
                "    print(f\"  F1 Score  : {f1:.4f}\")\n",
                "    print(f\"  MCC Score : {mcc:.4f}\")\n",
                "    print(\"=\" * 55)\n",
                "\n",
                "    # Classification Report\n",
                "    print(\"\\nClassification Report:\")\n",
                "    print(classification_report(y_test, y_pred, target_names=class_names))\n",
                "\n",
                "    # Confusion Matrix Heatmap\n",
                "    cm = confusion_matrix(y_test, y_pred)\n",
                "    plt.figure(figsize=(8, 6))\n",
                "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
                "                xticklabels=class_names,\n",
                "                yticklabels=class_names)\n",
                "    plt.title(f'Confusion Matrix - {model_name}')\n",
                "    plt.xlabel('Predicted')\n",
                "    plt.ylabel('Actual')\n",
                "    plt.show()\n",
                "\n",
                "    # Store results\n",
                "    results[model_name] = {\n",
                "        'Accuracy': acc,\n",
                "        'AUC Score': auc,\n",
                "        'Precision': prec,\n",
                "        'Recall': rec,\n",
                "        'F1 Score': f1,\n",
                "        'MCC Score': mcc\n",
                "    }\n",
                "\n",
                "    return results[model_name]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Model 1 - Logistic Regression"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.linear_model import LogisticRegression\n",
                "\n",
                "log_reg = LogisticRegression(\n",
                "    solver='lbfgs',\n",
                "    max_iter=1000,\n",
                "    random_state=42\n",
                ")\n",
                "log_reg.fit(X_train_scaled, y_train)\n",
                "\n",
                "# Save model\n",
                "joblib.dump(log_reg, 'Models/Logistic_Regression.joblib')\n",
                "print('Model saved to Models/Logistic_Regression.joblib')\n",
                "\n",
                "evaluate_model('Logistic Regression', log_reg, X_test_scaled, y_test)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Model 2 - Decision Tree Classifier"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.tree import DecisionTreeClassifier\n",
                "\n",
                "dt_clf = DecisionTreeClassifier(\n",
                "    criterion='gini',\n",
                "    max_depth=10,\n",
                "    min_samples_split=10,\n",
                "    min_samples_leaf=5,\n",
                "    random_state=42\n",
                ")\n",
                "dt_clf.fit(X_train_scaled, y_train)\n",
                "\n",
                "# Save model\n",
                "joblib.dump(dt_clf, 'Models/Decision_Tree.joblib')\n",
                "print('Model saved to Models/Decision_Tree.joblib')\n",
                "\n",
                "evaluate_model('Decision Tree', dt_clf, X_test_scaled, y_test)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Model 3 - K-Nearest Neighbor Classifier"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "\n",
                "knn_clf = KNeighborsClassifier(\n",
                "    n_neighbors=5,\n",
                "    weights='uniform',\n",
                "    metric='minkowski'\n",
                ")\n",
                "knn_clf.fit(X_train_scaled, y_train)\n",
                "\n",
                "# Save model\n",
                "joblib.dump(knn_clf, 'Models/KNN.joblib')\n",
                "print('Model saved to Models/KNN.joblib')\n",
                "\n",
                "evaluate_model('K-Nearest Neighbors', knn_clf, X_test_scaled, y_test)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Model 4 - Naive Bayes Classifier (Gaussian)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.naive_bayes import GaussianNB\n",
                "\n",
                "gnb_clf = GaussianNB()\n",
                "gnb_clf.fit(X_train_scaled, y_train)\n",
                "\n",
                "# Save model\n",
                "joblib.dump(gnb_clf, 'Models/Gaussian_Naive_Bayes.joblib')\n",
                "print('Model saved to Models/Gaussian_Naive_Bayes.joblib')\n",
                "\n",
                "evaluate_model('Gaussian Naive Bayes', gnb_clf, X_test_scaled, y_test)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Model 5 - Ensemble: Random Forest"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.ensemble import RandomForestClassifier\n",
                "\n",
                "rf_clf = RandomForestClassifier(\n",
                "    n_estimators=100,\n",
                "    max_depth=15,\n",
                "    min_samples_split=5,\n",
                "    min_samples_leaf=2,\n",
                "    random_state=42\n",
                ")\n",
                "rf_clf.fit(X_train_scaled, y_train)\n",
                "\n",
                "# Save model\n",
                "joblib.dump(rf_clf, 'Models/Random_Forest.joblib')\n",
                "print('Model saved to Models/Random_Forest.joblib')\n",
                "\n",
                "evaluate_model('Random Forest', rf_clf, X_test_scaled, y_test)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Model 6 - Ensemble: XGBoost"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from xgboost import XGBClassifier\n",
                "\n",
                "xgb_clf = XGBClassifier(\n",
                "    n_estimators=100,\n",
                "    max_depth=6,\n",
                "    learning_rate=0.1,\n",
                "    use_label_encoder=False,\n",
                "    eval_metric='mlogloss',\n",
                "    random_state=42\n",
                ")\n",
                "xgb_clf.fit(X_train_scaled, y_train)\n",
                "\n",
                "# Save model\n",
                "joblib.dump(xgb_clf, 'Models/XGBoost.joblib')\n",
                "print('Model saved to Models/XGBoost.joblib')\n",
                "\n",
                "evaluate_model('XGBoost', xgb_clf, X_test_scaled, y_test)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Model Comparison Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a comparison DataFrame\n",
                "comparison_df = pd.DataFrame(results).T\n",
                "comparison_df = comparison_df.round(4)\n",
                "print(\"\\n\" + \"=\" * 80)\n",
                "print(\"  MODEL COMPARISON - ALL 6 CLASSIFIERS\")\n",
                "print(\"=\" * 80)\n",
                "print(comparison_df.to_string())\n",
                "print(\"=\" * 80)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize the comparison\n",
                "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
                "fig.suptitle('Model Comparison Across All Metrics', fontsize=16, fontweight='bold')\n",
                "\n",
                "metrics = ['Accuracy', 'AUC Score', 'Precision', 'Recall', 'F1 Score', 'MCC Score']\n",
                "colors = ['#2196F3', '#4CAF50', '#FF9800', '#E91E63', '#9C27B0', '#00BCD4']\n",
                "\n",
                "for idx, (metric, color) in enumerate(zip(metrics, colors)):\n",
                "    ax = axes[idx // 3][idx % 3]\n",
                "    values = [results[model][metric] for model in results]\n",
                "    model_names = list(results.keys())\n",
                "    bars = ax.barh(model_names, values, color=color, alpha=0.8)\n",
                "    ax.set_title(metric, fontsize=12, fontweight='bold')\n",
                "    ax.set_xlim(0, 1.05)\n",
                "    for bar, val in zip(bars, values):\n",
                "        ax.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,\n",
                "                f'{val:.4f}', va='center', fontsize=9)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Identify the best model for each metric\n",
                "print(\"\\nBest Model per Metric:\")\n",
                "print(\"-\" * 40)\n",
                "for metric in metrics:\n",
                "    best_model = max(results, key=lambda m: results[m][metric])\n",
                "    best_value = results[best_model][metric]\n",
                "    print(f\"  {metric:12s} : {best_model} ({best_value:.4f})\")\n",
                "\n",
                "# Overall best model (by average rank across all metrics)\n",
                "print(\"\\n\" + \"=\" * 40)\n",
                "avg_scores = {model: np.mean(list(vals.values())) for model, vals in results.items()}\n",
                "best_overall = max(avg_scores, key=avg_scores.get)\n",
                "print(f\"Best Overall Model: {best_overall} (avg score: {avg_scores[best_overall]:.4f})\")\n",
                "print(\"=\" * 40)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# List all saved models\n",
                "print(\"\\nSaved Models in 'Models/' folder:\")\n",
                "for f in os.listdir('Models'):\n",
                "    print(f\"  - {f}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.12.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}